# -*- coding: utf-8 -*-
"""Customer Online Loan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zMv2ZuibE7IqW3qBZjHyR1Khlxb3jq26
"""

from google.colab import drive
drive.mount('/content/drive')

pip install dython

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

print('Numpy Version:', np.__version__)
print('Pandas Version:', pd.__version__)
print('Seaborn Version:', sns.__version__)

from matplotlib import rcParams
rcParams['figure.figsize'] = 12, 4
rcParams['lines.linewidth'] = 3
rcParams['xtick.labelsize'] = 'x-large'
rcParams['ytick.labelsize'] = 'x-large'

df1= pd.read_csv('/content/drive/My Drive/Virtual Internship - Home Credit Indonesia - Customer Online Loan/application_train.csv')

df1

df6=df1[['SK_ID_CURR','TARGET','NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',
                   'CNT_CHILDREN','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE',
                   'NAME_TYPE_SUITE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS',
                   'NAME_HOUSING_TYPE','REGION_POPULATION_RELATIVE','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION',
                   'FLAG_MOBIL','FLAG_EMP_PHONE','FLAG_WORK_PHONE','FLAG_CONT_MOBILE','FLAG_PHONE','FLAG_EMAIL',
                   'OCCUPATION_TYPE','CNT_FAM_MEMBERS','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY',
                   'WEEKDAY_APPR_PROCESS_START','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION',
                   'LIVE_REGION_NOT_WORK_REGION','REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY','LIVE_CITY_NOT_WORK_CITY',
                   'ORGANIZATION_TYPE','AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY',
                   'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']]

df6.info()

df2= pd.read_csv('/content/drive/My Drive/Virtual Internship - Home Credit Indonesia - Customer Online Loan/previous_application.csv')

df2.info()

df5=df2[['SK_ID_PREV','SK_ID_CURR','AMT_APPLICATION','AMT_DOWN_PAYMENT','FLAG_LAST_APPL_PER_CONTRACT',
         'NFLAG_LAST_APPL_IN_DAY','RATE_DOWN_PAYMENT','RATE_INTEREST_PRIMARY','RATE_INTEREST_PRIVILEGED',
         'NAME_CASH_LOAN_PURPOSE','NAME_CONTRACT_STATUS','DAYS_DECISION','NAME_PAYMENT_TYPE','CODE_REJECT_REASON',
         'NAME_CLIENT_TYPE','NAME_GOODS_CATEGORY','NAME_PORTFOLIO','NAME_PRODUCT_TYPE','CHANNEL_TYPE',
         'SELLERPLACE_AREA','NAME_SELLER_INDUSTRY','CNT_PAYMENT','NAME_YIELD_GROUP','PRODUCT_COMBINATION','DAYS_FIRST_DRAWING',
         'DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE','DAYS_TERMINATION','NFLAG_INSURED_ON_APPROVAL']]

df5.info()

df1 = pd.read_csv('application_train.csv')

df2 = pd.read_csv('previous_application.csv')

df=df6.merge(df5, on='SK_ID_CURR', how='inner')

df

df.info()

df.sample(5)



"""## Exploratory Data Analysis

### Statistical Descriptive
"""

df.info()

df.sample(5)

df['TARGET'] = df['TARGET'].replace({0:False, 1:True})
df['FLAG_EMP_PHONE'] = df['FLAG_EMP_PHONE'].replace({0:False, 1:True})
df['FLAG_PHONE'] = df['FLAG_PHONE'].replace({0:False, 1:True})
df['FLAG_EMAIL'] = df['FLAG_EMAIL'].replace({0:False, 1:True})

df.info()

"""Hasil Pengamatan:
1. Teridiri dari 72 kolom
2. memiliki 1413701 baris
3. Memiliki tipe data float, integer, dan object
4.Beberapa kolom memiliki data kosong
5. Pada kolom Target, tipe datanya diubah dari int menjadi bool. 0: False dan 1: True
6. Pada kolom Flag EMP Phone,tipe datanya diubah dari int menjadi bool. 0: False dan 1: True
7. Pada kolom Flag Phone,tipe datanya diubah dari int menjadi bool. 0: False dan 1: True
8. Pada kolom Flag Email,tipe datanya diubah dari int menjadi bool. 0: False dan 1: True

### Cuplikan Data
"""

df.sample(5)

"""### Statistical Summary"""

nums = ['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','CNT_FAM_MEMBERS','CNT_PAYMENT','DAYS_FIRST_DUE','DAYS_LAST_DUE','AMT_GOODS_PRICE','DAYS_EMPLOYED','CNT_CHILDREN']
cats = ['TARGET','NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_INCOME_TYPE','OCCUPATION_TYPE','ORGANIZATION_TYPE','NAME_HOUSING_TYPE','CODE_REJECT_REASON','FLAG_PHONE','FLAG_EMAIL','FLAG_EMP_PHONE']

df[nums].describe()

df[cats].describe()

"""### Counting Values


"""

for col in cats:
    print(f'''Value count kolom {col}:''')
    print(df[col].value_counts())
    print()

"""### Univariate Analysis"""

features = nums
plt.figure(figsize=(8, 10))
for i in range(0, len(features)):
    plt.subplot(4, 4, i+1)
    sns.boxplot(y=df[features[i]], color='green', orient='v')
    plt.tight_layout()

features = nums
plt.figure(figsize=(12, 10))
for i in range(0, len(nums)):
    plt.subplot(4, 4, i+1)
    sns.kdeplot(x=df[features[i]], color='blue')
    plt.xlabel(features[i])
    plt.tight_layout()

plt.figure(figsize=(12, 10))
for i in range(0, len(cats)):
    plt.subplot(6, 3, i+1)
    sns.countplot(x = df[cats[i]], color='orange', orient='v')
    plt.tight_layout()



"""### Bivariate Analysis"""

df.corr()

# correlation heatmap
plt.figure(figsize=(10, 10))
sns.heatmap(df.corr(), cmap='Blues', annot=True, fmt='.2f')

plt.figure(figsize=(20, 20))
sns.pairplot(df[nums], diag_kind='kde')

from dython.nominal import associations, identify_nominal_columns
associations(df[['TARGET','NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_INCOME_TYPE','OCCUPATION_TYPE','ORGANIZATION_TYPE','NAME_HOUSING_TYPE','CODE_REJECT_REASON','FLAG_PHONE','FLAG_EMAIL','FLAG_EMP_PHONE']], figsize=(10,10), nan_strategy='drop_samples')
plt.savefig('correlation.jpeg')

features = cats
plt.figure(figsize=(10, 25))
for i in range(0, len(features)):
    plt.subplot(15, 1, i+1) 
    sns.countplot(x=features[i], data=df,  palette="seismic", hue="TARGET")
    #plt.xlabel(features[i])
    plt.tight_layout()

features = nums
plt.figure(figsize=(10, 25))
for i in range(0, len(features)):
    plt.subplot(10, 1, i+1) 
    sns.countplot(x=features[i], data=df,  palette="seismic", hue="TARGET")
    #plt.xlabel(features[i])
    plt.tight_layout()





"""## Data Preprocessing

### Data Cleansing
"""

nums = ['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','CNT_FAM_MEMBERS','CNT_PAYMENT','DAYS_FIRST_DUE','DAYS_LAST_DUE','AMT_GOODS_PRICE','DAYS_EMPLOYED','CNT_CHILDREN']
cats = ['TARGET','NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_INCOME_TYPE','OCCUPATION_TYPE','ORGANIZATION_TYPE','NAME_HOUSING_TYPE','CODE_REJECT_REASON','FLAG_PHONE','FLAG_EMAIL','FLAG_EMP_PHONE']

df.isna().sum()

df=df.drop(columns=['DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE','DAYS_TERMINATION','NFLAG_INSURED_ON_APPROVAL'])

df.isna().sum()

df=df.drop(columns=['PRODUCT_COMBINATION','DAYS_FIRST_DRAWING'])

df['CNT_PAYMENT']=df['CNT_PAYMENT'].fillna(df['CNT_PAYMENT'].mean())

df.isna().sum()

df=df.drop(columns=['AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR','SK_ID_PREV','NAME_TYPE_SUITE','RATE_DOWN_PAYMENT','RATE_INTEREST_PRIMARY','RATE_INTEREST_PRIVILEGED','AMT_DOWN_PAYMENT'])

df['AMT_ANNUITY']=df['AMT_ANNUITY'].fillna(df['AMT_ANNUITY'].mean())
df['AMT_GOODS_PRICE']=df['AMT_GOODS_PRICE'].fillna(df['AMT_GOODS_PRICE'].mean())
df['OCCUPATION_TYPE']=df['OCCUPATION_TYPE'].fillna('Laborers')

df=df.drop(columns=['SK_ID_CURR'])

df.info()

df.duplicated().sum()

df3=df.drop_duplicates()

df3.duplicated().sum()

nums = ['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','CNT_FAM_MEMBERS','CNT_PAYMENT','DAYS_FIRST_DUE','DAYS_LAST_DUE','AMT_GOODS_PRICE','DAYS_EMPLOYED','CNT_CHILDREN']

df3['log_AMT_INCOME_TOTAL']= np.log(df3['AMT_INCOME_TOTAL']+np.finfo(float).eps)
df3['log_AMT_CREDIT']= np.log(df3['AMT_CREDIT']+np.finfo(float).eps)
df3['log_AMT_ANNUITY']= np.log(df3['AMT_ANNUITY']+np.finfo(float).eps)
df3['log_CNT_FAM_MEMBERS']= np.log(df3['CNT_FAM_MEMBERS']+np.finfo(float).eps)
df3['log_CNT_PAYMENT']= np.log(df3['CNT_PAYMENT']+np.finfo(float).eps)
df3['log_AMT_GOODS_PRICE']= np.log(df3['AMT_GOODS_PRICE']+np.finfo(float).eps)
df3['log_DAYS_EMPLOYED']= np.log(df3['DAYS_EMPLOYED']+np.finfo(float).eps)
df3['log_CNT_CHILDREN']= np.log(df3['CNT_CHILDREN']+np.finfo(float).eps)

log=['log_AMT_INCOME_TOTAL','log_AMT_CREDIT','log_AMT_ANNUITY','log_CNT_FAM_MEMBERS','log_CNT_PAYMENT','log_AMT_GOODS_PRICE','log_DAYS_EMPLOYED','log_CNT_CHILDREN']

features = log
plt.figure(figsize=(12, 10))
for i in range(0, len(log)):
    plt.subplot(4, 2, i+1)
    sns.kdeplot(x=df3[features[i]], color='green')
    plt.xlabel(features[i])
    plt.tight_layout()

df4 = df3.drop(columns=['AMT_INCOME_TOTAL', 'AMT_CREDIT','AMT_ANNUITY','CNT_FAM_MEMBERS','log_CNT_PAYMENT','AMT_GOODS_PRICE','log_DAYS_EMPLOYED','CNT_CHILDREN'])

df4.info()

nums1=['log_AMT_INCOME_TOTAL','log_AMT_CREDIT','log_AMT_ANNUITY','log_CNT_FAM_MEMBERS','CNT_PAYMENT','log_AMT_GOODS_PRICE','DAYS_EMPLOYED','log_CNT_CHILDREN']

from scipy import stats
print(f'Jumlah baris sebelum memfilter outlier: {len(df4)}')

filtered_entries = np.array([True] * len(df4))

for col in nums1:
    zscore = abs(stats.zscore(df3[col])) # hitung absolute z-scorenya
    filtered_entries = (zscore < 3) & filtered_entries # keep yang kurang dari 3 absolute z-scorenya
    
df7 = df4[filtered_entries] # filter, cuma ambil yang z-scorenya dibawah 3

print(f'Jumlah baris setelah memfilter outlier: {len(df7)}')

df7.info()

df7 = df7.drop(columns=['REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','WEEKDAY_APPR_PROCESS_START','REG_REGION_NOT_LIVE_REGION',
                        'REG_REGION_NOT_WORK_REGION','LIVE_REGION_NOT_WORK_REGION','REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY',
                        'LIVE_CITY_NOT_WORK_CITY','NAME_CASH_LOAN_PURPOSE','NAME_CASH_LOAN_PURPOSE','NAME_PORTFOLIO','SELLERPLACE_AREA','NAME_SELLER_INDUSTRY',
                        'NAME_YIELD_GROUP','CHANNEL_TYPE','NAME_CLIENT_TYPE','NAME_PRODUCT_TYPE','CHANNEL_TYPE'])

df7.info()

"""### Feature Encoding"""

df7['TARGET'].unique()

df7['TARGET']=df7['TARGET'].astype(int)

df7.info()

mapping_contract={'Cash loans':0, 'Revolving loans':1}
df7['NAME_CONTRACT_TYPE']=df7['NAME_CONTRACT_TYPE'].map(mapping_contract)

df7.info()

df7['CODE_GENDER'].replace('XNA', 'F', inplace=True)

mapping_GENDER={'F':0, 'M':1}
df7['CODE_GENDER']=df7['CODE_GENDER'].map(mapping_GENDER)

df7.info()

mapping_CAR={'N':0, 'Y':1}
df7['FLAG_OWN_CAR']=df7['FLAG_OWN_CAR'].map(mapping_CAR)

df7.info()

mapping_REALTY={'N':0, 'Y':1}
df7['FLAG_OWN_REALTY']=df7['FLAG_OWN_REALTY'].map(mapping_REALTY)

df7.info()

mapping_EDUCATION={'Incomplete higher':0, 'Lower secondary':1, 'Secondary / secondary special':2, 'Academic degree':3, 'Higher education':4}
df7['NAME_EDUCATION_TYPE']=df7['NAME_EDUCATION_TYPE'].map(mapping_EDUCATION)

df7.info()

df7['FLAG_PHONE']=df7['FLAG_PHONE'].astype(int)

df7.info()

df7['FLAG_EMAIL']=df7['FLAG_EMAIL'].astype(int)

df7.info()

df7['FLAG_EMP_PHONE']=df7['FLAG_EMP_PHONE'].astype(int)

df7.info()

mapping_lastap={'N':0, 'Y':1}
df7['FLAG_LAST_APPL_PER_CONTRACT']=df7['FLAG_LAST_APPL_PER_CONTRACT'].map(mapping_lastap)

df7.info()

cats_one = ['NAME_INCOME_TYPE','NAME_FAMILY_STATUS','OCCUPATION_TYPE','ORGANIZATION_TYPE','CODE_REJECT_REASON','NAME_HOUSING_TYPE','NAME_CONTRACT_STATUS','NAME_PAYMENT_TYPE','NAME_GOODS_CATEGORY']
for cats_one in  ['NAME_INCOME_TYPE','NAME_FAMILY_STATUS','OCCUPATION_TYPE','ORGANIZATION_TYPE','CODE_REJECT_REASON','NAME_HOUSING_TYPE','NAME_CONTRACT_STATUS','NAME_PAYMENT_TYPE','NAME_GOODS_CATEGORY']:
    onehots = pd.get_dummies(df7[cats_one], prefix=cats_one)
    df7 = df7.join(onehots)

df8=df7.drop(columns=['NAME_INCOME_TYPE','NAME_FAMILY_STATUS','OCCUPATION_TYPE','ORGANIZATION_TYPE','CODE_REJECT_REASON','NAME_HOUSING_TYPE','NAME_CONTRACT_STATUS','NAME_PAYMENT_TYPE','NAME_GOODS_CATEGORY'])

df8.info()

"""### Feature Selection"""

corrmat = df8.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(50,50))
g=sns.heatmap(df8[top_corr_features].corr(),annot=True,cmap="RdYlGn")

a = corrmat['TARGET'] #mengambil nilai korelasi Revenue
hasil = a[(a>0.04)|(a<-0.04)]
hasil

df8_selection=df8[['TARGET','CODE_GENDER','NAME_EDUCATION_TYPE','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','FLAG_EMP_PHONE','NAME_INCOME_TYPE_Pensioner','NAME_INCOME_TYPE_Working','ORGANIZATION_TYPE_XNA','CODE_REJECT_REASON_XAP','NAME_CONTRACT_STATUS_Approved','NAME_CONTRACT_STATUS_Refused']]



"""## Modelling and Evaluation

### Split Data
"""

# Split Feature and Label
X = df8_selection[['CODE_GENDER','NAME_EDUCATION_TYPE','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','FLAG_EMP_PHONE','NAME_INCOME_TYPE_Pensioner','NAME_INCOME_TYPE_Working','ORGANIZATION_TYPE_XNA','CODE_REJECT_REASON_XAP','NAME_CONTRACT_STATUS_Approved','NAME_CONTRACT_STATUS_Refused']]
y = df8_selection['TARGET'] # target / label

#Splitting the data into Train and Test
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

df8_selection['TARGET'].value_counts()

"""### Class Imbalance"""

from imblearn import under_sampling, over_sampling
X_under, y_under = under_sampling.RandomUnderSampler().fit_resample(X_train, y_train)
X_over, y_over = over_sampling.RandomOverSampler().fit_resample(X_train, y_train)
X_over_SMOTE, y_over_SMOTE = over_sampling.SMOTE().fit_resample(X_train, y_train)

print('Original')
print(pd.Series(y).value_counts())
print('\n')
print('UNDERSAMPLING')
print(pd.Series(y_under).value_counts())
print('\n')
print('OVERSAMPLING')
print(pd.Series(y_over).value_counts())
print('\n')
print('SMOTE')
print(pd.Series(y_over_SMOTE).value_counts())



"""### Model Evaluation"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def eval_classification(model):
    y_pred = model.predict(X_test)
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_pred))
    print("Precision (Test Set): %.2f" % precision_score(y_test, y_pred))
    print("Recall (Test Set): %.2f" % recall_score(y_test, y_pred))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_pred))
    print('AUC:'+ str(roc_auc_score(y_test, y_pred)))

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])

"""#### 1. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=42)
lr.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(lr)

print('Train score: ' + str(lr.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score: ' + str(lr.score(X_test, y_test))) #accuracy



"""##### Tuning Hyperparameters"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# List Hyperparameters yang akan diuji
solver = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2','l1', 'elasticnet', 'none']
C = [100, 10, 1.0, 0.1, 0.01, 0.001, 0.0001]
hyperparameters = dict(penalty= bpenalty, C=C, solver=solver )

# Inisiasi model
logres = LogisticRegression(random_state=42) # Init Logres dengan Gridsearch, cross validation = 5
lr_tuned = RandomizedSearchCV(logres, hyperparameters, cv=5, random_state=42, scoring='recall')

# Fitting Model & Evaluation
lr_tuned.fit(X_over_SMOTE, y_over_SMOTE)
eval_classification(lr_tuned)

print('Train score: ' + str(lr.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score: ' + str(lr.score(X_test, y_test))) #accuracy

logres = LogisticRegression(penalty='l2', C=0.0001, solver='lbfgs', random_state=42)
logres.fit(X_over_SMOTE, y_over_SMOTE)
y_pred = logres.predict(X_test)

import math

feature_names = X_over_SMOTE.columns.to_list()

#Get the scores
score = logres.score(X_over_SMOTE.values, y_over_SMOTE)
print(score)
w0 = logres.intercept_[0]
w = logres.coef_[0]

feature_importance = pd.DataFrame(feature_names, columns = ['feature'])
feature_importance['importance'] = pow(math.e, w)
feature_importance = feature_importance.sort_values(by=['importance'],ascending=False)
feature_importance = feature_importance[:8].sort_values(by=['importance'], ascending=False)

#Visualization
ax = feature_importance.sort_values(by=['importance'], ascending=True).plot.barh(x='feature', y='importance')

plt.title('Important Feature')
plt.show()

print('Train score: ' + str(lr_tuned.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(lr_tuned.score(X_test, y_test))) #accuracy

"""#### 2. Random Forest"""

from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42)
rf.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(rf)

print('Train score: ' + str(rf.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(rf.score(X_test, y_test))) #accuracy

"""##### Tuning Hyperparameters"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

#List Hyperparameters yang akan diuji
hyperparameters = dict(
                       n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)], # Jumlah subtree 
                       bootstrap = [True], # Apakah pakai bootstrapping atau tidak
                       criterion = ['gini','entropy'],
                       max_depth = [int(x) for x in np.linspace(10, 110, num = 11)],  # Maximum kedalaman tree
                       min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)], # Jumlah minimum samples pada node agar boleh di split menjadi leaf baru
                       min_samples_leaf = [int(x) for x in np.linspace(start = 1, stop = 10, num = 5)], # Jumlah minimum samples pada leaf agar boleh terbentuk leaf baru
                       max_features = ['auto', 'sqrt', 'log2'], # Jumlah feature yg dipertimbangkan pada masing-masing split
                       n_jobs = [-1], # Core untuk parallel computation. -1 untuk menggunakan semua core
                      )
 
# Init
rf = RandomForestClassifier(random_state=42)
rf_tuned = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=42, scoring='recall')
rf_tuned.fit(X_over_SMOTE, y_over_SMOTE)

# Predict & Evaluation
eval_classification(rf_tuned)

feat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)
ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

"""### 3. Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(dt)

print('Train score: ' + str(dt.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(dt.score(X_test, y_test))) #accuracy

"""##### Tuning Hyperparameters"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
import numpy as np

# List of hyperparameter
max_depth = [int(x) for x in np.linspace(1, 110, num = 30)] # Maximum number of levels in tree
min_samples_split = [2, 5, 10, 100] # Minimum number of samples required to split a node
min_samples_leaf = [1, 2, 4, 10, 20, 50] # Minimum number of samples required at each leaf node
max_features = ['auto', 'sqrt'] # Number of features to consider at every split

hyperparameters = dict(max_depth=max_depth, 
                       min_samples_split=min_samples_split, 
                       min_samples_leaf=min_samples_leaf,
                       max_features=max_features
                      )

# Inisialisasi Model
dt = DecisionTreeClassifier(random_state=42)
dt_tuned = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=42, scoring='precision')
dt_tuned.fit(X_over_SMOTE, y_over_SMOTE)

# Predict & Evaluation
eval_classification(dt_tuned)

feat_importances = pd.Series(dt.feature_importances_, index=X_train.columns)
ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

plt.xlabel('score')
plt.ylabel('feature')
plt.title('feature importance score')

plt.savefig('futureimportanceab.jpeg', dpi=200)



"""#### 4. Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
ab = AdaBoostClassifier(random_state=42)
ab.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(ab)

print('Train score: ' + str(ab.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(ab.score(X_test, y_test))) #accuracy



"""##### Tuning Hyperparameters"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

# List of hyperparameter
hyperparameters = dict(n_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 2000)], # Jumlah iterasi
                       learning_rate = [float(x) for x in np.linspace(start = 0.001, stop = 0.1, num = 200)],  
                       algorithm = ['SAMME', 'SAMME.R']
                      )

# Init model
ab = AdaBoostClassifier(random_state=42)
ab_tuned = RandomizedSearchCV(ab, hyperparameters, random_state=42, cv=5, scoring='recall')
ab_tuned.fit(X_over_SMOTE, y_over_SMOTE)

# Predict & Evaluation
eval_classification(ab_tuned)

print('Train score: ' + str(ab_tuned.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score: ' + str(ab_tuned.score(X_test, y_test))) #accuracy

feat_importances = pd.Series(ab.feature_importances_, index=X_train.columns)
ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

plt.xlabel('score')
plt.ylabel('feature')
plt.title('feature importance score')

plt.savefig('futureimportanceab.jpeg', dpi=200)

"""#### 5. XGBoost"""

from xgboost import XGBClassifier
xg = XGBClassifier(random_state=42)
xg.fit(X_over_SMOTE, y_over_SMOTE)

eval_classification(xg)

print('Train score: ' + str(xg.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(xg.score(X_test, y_test))) #accuracy

show_feature_importance(xg)

"""##### Tuning Hyperparameters"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import numpy as np

#Menjadikan ke dalam bentuk dictionary
hyperparameters = {
                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],
                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],
                    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],

                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],

                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],
                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]
                    }

# Init
xg = XGBClassifier(random_state=42)
xg_tuned = RandomizedSearchCV(xg, hyperparameters, cv=5, random_state=42, scoring='recall')
xg_tuned.fit(X_over_SMOTE, y_over_SMOTE)

# Predict & Evaluation
eval_classification(xg_tuned)

print('Train score: ' + str(xg_tuned.score(X_over_SMOTE, y_over_SMOTE))) #accuracy
print('Test score:' + str(xg_tuned.score(X_test, y_test))) #accuracy

feat_importances = pd.Series(xg.feature_importances_, index=X_train.columns)
ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
ax.invert_yaxis()

plt.xlabel('score')
plt.ylabel('feature')
plt.title('feature importance score')

plt.savefig('futureimportanceab.jpeg', dpi=200)

"""### Best Model"""

# Split Feature and Label
X_imp = X_over_SMOTE[['DAYS_BIRTH','DAYS_REGISTRATION','DAYS_EMPLOYED','NAME_EDUCATION_TYPE','NAME_CONTRACT_STATUS_Refused','NAME_CONTRACT_STATUS_Approved','CODE_REJECT_REASON_XAP','CODE_GENDER','NAME_INCOME_TYPE_Working']]
y_imp = y_over_SMOTE # target / label

#Splitting the data into Train and Test
from sklearn.model_selection import train_test_split 
X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(X_imp, y_imp, test_size = 0.3, random_state = 42)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def eval_class(model):
    y_pred_imp = model.predict(X_test_imp)
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test_imp, y_pred_imp))
    print("Precision (Test Set): %.2f" % precision_score(y_test_imp, y_pred_imp))
    print("Recall (Test Set): %.2f" % recall_score(y_test_imp, y_pred_imp))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test_imp, y_pred_imp))
    print('AUC:'+ str(roc_auc_score(y_test_imp, y_pred_imp)))

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])

from sklearn.ensemble import RandomForestClassifier
rf_imp = RandomForestClassifier(random_state=42)
rf_imp.fit(X_train_imp,y_train_imp)

eval_class(rf_imp)

from sklearn.metrics import confusion_matrix

#Generate the confusion matrix
y_pred_c = rf_imp.predict(X_test_imp)
cf_matrix = confusion_matrix(y_test_imp, y_pred_c)

print(cf_matrix)

group_names = ['True Negative','False Positive','False Negative','True Positive']
group_counts = ["{0:0.0f}".format(value) for value in
cf_matrix.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')
ax.set_title('Confusion Matrix\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True'])
## Display the visualization of the Confusion Matrix.
plt.show()

# Split Feature and Label
X_imp1 = X_over_SMOTE[['DAYS_BIRTH','DAYS_REGISTRATION','DAYS_EMPLOYED','NAME_EDUCATION_TYPE','NAME_CONTRACT_STATUS_Refused','NAME_CONTRACT_STATUS_Approved','CODE_REJECT_REASON_XAP','CODE_GENDER','NAME_INCOME_TYPE_Working']]
y_imp1 = y_over_SMOTE # target / label

#Splitting the data into Train and Test
from sklearn.model_selection import train_test_split 
X_train_imp1, X_test_imp1, y_train_imp1, y_test_imp1 = train_test_split(X_imp1, y_imp1, test_size = 0.3, random_state = 42)

from sklearn.tree import DecisionTreeClassifier
dt_imp1 = DecisionTreeClassifier(random_state=42)
dt_imp1.fit(X_train_imp1,y_train_imp1)

eval_class(dt_imp1)

from sklearn.metrics import confusion_matrix

#Generate the confusion matrix
y_pred_c = dt_imp1.predict(X_test_imp1)
cf_matrix = confusion_matrix(y_test_imp1, y_pred_c)

print(cf_matrix)

group_names = ['True Negative','False Positive','False Negative','True Positive']
group_counts = ["{0:0.0f}".format(value) for value in
cf_matrix.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in
cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')
ax.set_title('Confusion Matrix\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True'])
## Display the visualization of the Confusion Matrix.
plt.show()

